# Replication package of the work: Deep Assessment of Code Review Generation Approaches: Beyond Lexical Similarity

In this paper, we explore how semantic similarity between generated and reference reviews can enhance the automated assessment of code reviews. We first present a benchmark called GradedReviews, which is constructed by collecting real-world code reviews from open-source projects, generating reviews using state-of-the-art approaches, and manually assessing their quality. We then evaluate existing metrics for code review assessment using this benchmark, revealing their limitations.

# Directory Structure
There are two folders within the replication package:

./Code: The source code of deep assessment of code review generation approach

./DataSet: Data to replicate the evaluation in the paper

# DataSet:
![Methodology for Benchmark Construction](./dataset.png "Methodology for Benchmark Construction")
